{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# OpenStreetMap Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step One - Complete Programming Exercises\n",
    "Make sure all programming exercises are solved correctly in the \"Case Study: OpenStreetMap Data\" Lesson in the course you have chosen (MongoDB or SQL). This is the last lesson in that section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "import csv\n",
    "import codecs\n",
    "import cerberus\n",
    "from street_suffix import map_street_suffix\n",
    "from audit import audit_addresses\n",
    "from update import update_addr\n",
    "\n",
    "osm_file = 'sample.osm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Iterative Parsing\n",
    "Your task is to use the iterative parsing to process the map file and find out not only what tags are there, but also how many, to get the feeling on how much of which data you can expect to have in the map.  Return a dictionary with the tag name as the key and number of times this tag can be encountered in the map as value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'int'>, {'node': 32697, 'nd': 38628, 'member': 500, 'tag': 20670, 'relation': 51, 'way': 3410, 'osm': 1})\n"
     ]
    }
   ],
   "source": [
    "tags = defaultdict(int)\n",
    "for event, element in ET.iterparse(osm_file):\n",
    "    tags[element.tag] += 1\n",
    "print tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Tag Types\n",
    "Your task is to explore the data a bit more.  Before you process the data and add it into your database, you should check the \"k\" value for each tag and see if there are any potential problems.  We have provided you with 3 regular expressions to check for certain patterns in the tags. As we saw in the quiz earlier, we would like to change the data\n",
    "model and expand the \"addr:street\" type of keys to a dictionary like this: {\"address\": {\"street\": \"Some value\"}} So, we have to see if we have such tags, and if we have any tags with problematic characters.\n",
    "\n",
    "Please complete the function 'key_type', such that we have a count of each of\n",
    "four tag categories in a dictionary:\n",
    "  - \"lower\", for tags that contain only lowercase letters and are valid,\n",
    "  - \"lower_colon\", for otherwise valid tags with a colon in their names,\n",
    "  - \"problemchars\", for tags with problematic characters, and\n",
    "  - \"other\", for other tags that do not fall into the other three categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'problemchars': 0, 'lower': 9913, 'other': 1798, 'lower_colon': 8959}\n"
     ]
    }
   ],
   "source": [
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        result1 = lower.search(element.attrib['k'])\n",
    "        result2 = lower_colon.search(element.attrib['k'])\n",
    "        result3 = problemchars.search(element.attrib['k'])\n",
    "        \n",
    "        if result1 is not None:\n",
    "            keys['lower'] += 1\n",
    "        elif result2 is not None:\n",
    "            keys['lower_colon'] += 1\n",
    "        elif result3 is not None:\n",
    "            keys['problemchars'] += 1\n",
    "        else:\n",
    "            keys['other'] += 1\n",
    "\n",
    "    return keys\n",
    "\n",
    "\n",
    "\n",
    "keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "for _, element in ET.iterparse(osm_file):\n",
    "    keys = key_type(element, keys)\n",
    "\n",
    "print keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exploring Users\n",
    "Your task is to explore the data a bit more.  The first task is a fun one - find out how many unique users have contributed to the map in this particular area!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766  Total users\n"
     ]
    }
   ],
   "source": [
    "users = set()\n",
    "for _, element in ET.iterparse(osm_file):\n",
    "    uid=element.get('uid')\n",
    "    if uid is not None:\n",
    "        users.add(uid)\n",
    "print len(users), \" Total users\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Improving Street Names\n",
    "Your task in this exercise has two steps:\n",
    "\n",
    "- audit the osm_file and change the variable 'mapping' to reflect the changes needed to fix the unexpected street types to the appropriate ones in the expected list.\n",
    "- write the update_name function, to actually fix the street name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addr: fields \n",
      "['addr:city', 'addr:street', 'addr:postcode', 'addr:housenumber', 'addr:state', 'addr:housename', 'addr:country', 'addr:suite', 'addr:housenumber_1', 'addr:interpolation']\n",
      "______________\n",
      "Tiger: fields \n",
      "['tiger:name_base', 'tiger:name_type', 'tiger:zip_left', 'tiger:zip_right', 'tiger:name_direction_prefix', 'tiger:name_base_1', 'tiger:name_type_1', 'tiger:zip_left_1', 'tiger:zip_right_1', 'tiger:name_direction_suffix', 'tiger:zip_left_2', 'tiger:zip_right_2', 'tiger:name_base_2', 'tiger:name_type_2', 'tiger:zip_left_3', 'tiger:zip_left_4', 'tiger:zip_right_3', 'tiger:name_direction_prefix_1', 'tiger:name_direction_suffix_1']\n"
     ]
    }
   ],
   "source": [
    "with open(osm_file, \"r\") as f:\n",
    "    addr_address_fields = []\n",
    "    tiger_address_fields = []\n",
    "    for event, elem in ET.iterparse(f, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if \"addr\" in tag.attrib['k'] and tag.attrib['k'] not in addr_address_fields:\n",
    "                    addr_address_fields.append(tag.attrib['k'])\n",
    "                elif (\"tiger:name\" in tag.attrib['k'] or \"tiger:zip\" in tag.attrib['k']) and \\\n",
    "                    tag.attrib['k'] not in tiger_address_fields:\n",
    "                    tiger_address_fields.append(tag.attrib['k'])\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "print \"Addr: fields \"                    \n",
    "print addr_address_fields\n",
    "print \"______________\"\n",
    "print \"Tiger: fields \"\n",
    "print tiger_address_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expected, mapping = map_street_suffix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Street Types: \n",
      "{'#3': set(['Main St #3']),\n",
      " '70': set(['State Route 70']),\n",
      " '73': set(['State Route 73']),\n",
      " 'Aly': set(['Aly']),\n",
      " 'Atreet': set(['Arch Atreet']),\n",
      " 'Audubon': set(['John James Audubon']),\n",
      " 'Ave': set(['Ave']),\n",
      " 'B': set(['Salem Ave Building B']),\n",
      " 'Blvd': set(['Blvd']),\n",
      " 'Brg': set(['Brg']),\n",
      " 'Centura': set(['Centura']),\n",
      " 'Chanticleer': set(['Chanticleer']),\n",
      " 'Cir': set(['Cir', 'Woodfield Cir']),\n",
      " 'Croft': set(['Kings Croft']),\n",
      " 'Ct': set(['Ct', 'Portsmouth Ct']),\n",
      " 'Dr': set(['Dr']),\n",
      " 'Expy': set(['Expy']),\n",
      " 'Hwy': set(['Hwy']),\n",
      " 'Ii': set(['The Woods Ii']),\n",
      " 'Ln': set(['Ln']),\n",
      " 'Pky': set(['Pky']),\n",
      " 'Pl': set(['Pl']),\n",
      " 'Rd': set([\"Arney's Mount Rd\", 'Rd', 'South Easton Rd']),\n",
      " 'Royal': set(['Five Crown Royal']),\n",
      " 'Sheffield': set(['Sheffield']),\n",
      " 'Sq': set(['Sq']),\n",
      " 'St': set(['Carson St', 'Green St', 'Spring Garden St', 'St']),\n",
      " 'St.': set(['N 24th St.']),\n",
      " 'Ter': set(['Ter']),\n",
      " 'Trl': set(['Trl']),\n",
      " 'West': set(['Coventry Circle West']),\n",
      " 'Woods': set(['The Woods']),\n",
      " 'avenue': set(['Ohio avenue']),\n",
      " 'st': set(['jackson st', 'livingston st', 'mercer st', 's broad st'])}\n",
      "-------------------------\n",
      "States Represented: \n",
      "['NJ', 'PA', 'pa']\n",
      "-------------------------\n",
      "Irregular Postcodes: \n",
      "['7500', '08043-1053', '08043-2271', '08043-2056', '08043-1178', '08043-4756', '08043-2500', '08043-1681', '08043-4664', '08043-2938', '08043-4845', '08043-2550', '08043-3708', '08043-4949', '08043-4762', '08043-1233', '08043-4134', '08043-2273', '08043-2155', '08043-4698', '08043-3682', '08043-1617', '08043-2007', '08043-4717', '08043-4817', '08043-2040', '08043-4714', '08043-2865', '08043-1216', '08043-4211', '08043-3454', '08043-1657', '08043-4734', '08043-2063', '08043-2177', '08043-1204', '08043-1819', '08043-1820', '08043-3001', '08043-2559', '08043-1145', '08043-2508', '08043-1678', '08043-1540', '08043-3019', '08043-1206', '08043-4333', '08043-3300', '08043-4646', '08043-1644', '08043-1217', '08043-1625', '08043-4656', '08043-2843', '08043-2855', '08043-4110', '08043-4325', '08043-1271', '08043-4747', '08043-4740', '08003-3812', '08003-2812', '08003-1531', '08003-2853', '08003-3833', '08003-3712', '08003-3212', '08003-2834', '08003-4803', '08003-4826', '08003-3409', '08003-3918', '08003-1449', '08003-2803', '08003-4404', '08003-4858', '08003-1906', '08003-3926', '08003-3546', '08003-1813', '08003-1830', '08003-4723', '08003-4712', '08003-4829', '08003-1943', '08003-4707', '08003-1559', '08003-4715', '08003-4724', '08003-2203', '08003-2244', '08003-1966', '08003-3463', '08003-3014', '08003-1344', '08003-3323', '08003-5142', '08003-3210', '08003-2949', '08003-1919', '08003-3013', '08003-4855', '08003-2202', '08003-1550', '08003-3352', '08003-4839', '08003-3915', '08003-1216', '08003-5167', '08003-3331', '08003-1536', '08003-1567', '08003-1214', '08003-1421', '08003-4011', '08003-1718', '08003-3237', '08003-3432', '08003-3737', '08003-1939', '08003-1964', '08003-1913', '08003-1101', '08003-3921', '08003-1215', '08003-3127', '08003-2704', '08003-2767', '08003-4710', '08003-2923', '08003-4806', '08003-1996', '08003-2848', '08003-3436', '08003-1226', '08003-1239', '08003-1552', '08003-2661', '08003-2678', '08003-1549', '08003-2615', '08003-2231', '08003-2122', '08003-2407', '08003-1326', '08003-1703', '08003-1227', '08003-1122', '08003-2240', '08003-2326', '08003-2118', '08003-1017', '08003-4906', '08003-3520', '08053-1120', '08053-2415', '08053-2713', '08053-5730', '08053-4611', '08053-4629', '08053-1410', '08053-3868', '08053-1366', '08053-2862', '08053-2833', '08053-2745', '08053-3746', '08053-5599', '08053-2722', '08053-1248', '08053-4189', '08053-5406', '08053-1112', '08053-1423', '08053-1026', '08053-1251', '08053-2020', '08053-5551', '08053-2499', '08053-1395', '08053-3601', '08053-2905', '08053-5374', '08053-3704', '08053-2109', '08053-5366', '08053-1450', '08053-7025', '08053-5006', '08053-2494', '08053-3625', '08053-1779', '08053-1812', '08053-4234', '08053-4717', '08053-2870', '08053-5025', '08053-5536', '08053-3609', '08053-1346', '08053-7150', '08053-3780', '08053-3621', '08053-4700', '08053-1049', '08053-1326', '08053-3709', '08053-7003', '08053-4604', '08053-1012', '08053-2105', '08053-4702', '08053-4140', '08053-1419', '08053-5345', '08053-7052', '08053-3745', '08053-4918', '08053-1387', '08053-3916', '08053-1934', '08053-2943', '08053-3772', '08053-4919', '08053-1151', '08053-9603', '08053-2852', '08053-3619', '08053-3749', '08053-4724', '08053-2919', '08053-1371', '08053-3717', '08053-4905', '08053-2417', '08053-3725', '08053-3603', '08053-2126', '08053-2758', '08053-4229', '08053-2901', '08053-3742', '08053-2525', '08053-4138', '08053-5203', '08053-1820', '08053-5604', '08053-4916', '08053-2451', '08053-5544', '08053-1949', '08053-9726', '08053-9658', '08053-8533', '08053-3848', '08053-1440', '08053-8528', '08053-1744', '08053-1214', '08053-2916', '08053-2824', '08053-3819', '08053-1965', '08053-4231', '08053-1932', '08053-2844', '08053-3911', '08053-5602', '08053-2094', '08053-1114', '08053-1462', '08053-2456', '08053-4256', '08053-2607', '08053-2143', '08053-4250', '08053-2753', '08053-5542', '08053-1389', '08053-1912', '08053-5501', '08053-1953', '08053-1979', '08053-1435', '08053-1382', '08053-1401', '08053-2460', '08053-3771', '08053-5108', '08053-1869', '08053-1024', '08053-3608', '08053-1047', '08053-3506', '08053-7155', '08053-1432', '08053-2918', '08053-4238', '08053-7104', '08053-5101', '19107;19133', '19124:19137', '08028:08062', '19119; 19140', '19126; 19138', '08053-1613', '08003-2207', '08003-2553', '08003-2243']\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "audit_addresses(osm_file, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Preparing for Database - SQL\n",
    "After auditing is complete the next step is to prepare the data to be inserted into a SQL database.  To do so you will parse the elements in the OSM XML file, transforming them from document format to tabular format, thus making it possible to write to .csv files.  These csv files can then easily be imported to a SQL database as tables.\n",
    "\n",
    "The process for this transformation is as follows:\n",
    "- Use iterparse to iteratively step through each top level element in the XML\n",
    "- Shape each element into several data structures using a custom function\n",
    "- Utilize a schema and validation library to ensure the transformed data is in the correct format\n",
    "- Write each data structure to the appropriate .csv files\n",
    "\n",
    "We've already provided the code needed to load the data, perform iterative parsing and write the output to csv files. Your task is to complete the shape_element function that will transform each element into the correct format. To make this process easier we've already defined a schema (see the schema.py file in the last code tab) for the .csv files and the eventual tables. Using the cerberus library we can validate the output against this schema to ensure it is correct.\n",
    "\n",
    "#### Shape Element Function\n",
    "The function should take as input an iterparse Element object and return a dictionary.\n",
    "\n",
    "##### If the element top level tag is \"node\":\n",
    "The dictionary returned should have the format {\"node\": .., \"node_tags\": ...}\n",
    "\n",
    "The \"node\" field should hold a dictionary of the following top level node attributes:\n",
    "- id\n",
    "- user\n",
    "- uid\n",
    "- version\n",
    "- lat\n",
    "- lon\n",
    "- timestamp\n",
    "- changeset\n",
    "All other attributes can be ignored\n",
    "\n",
    "The \"node_tags\" field should hold a list of dictionaries, one per secondary tag. Secondary tags are\n",
    "child tags of node which have the tag name/type: \"tag\". Each dictionary should have the following\n",
    "fields from the secondary tag attributes:\n",
    "\n",
    "- id: the top level node id attribute value\n",
    "- key: the full tag \"k\" attribute value if no colon is present or the characters after the colon if one is.\n",
    "- value: the tag \"v\" attribute value\n",
    "- type: either the characters before the colon in the tag \"k\" value or \"regular\" if a colon is not present.\n",
    "\n",
    "Additionally,\n",
    "\n",
    "- if the tag \"k\" value contains problematic characters, the tag should be ignored\n",
    "- if the tag \"k\" value contains a \":\" the characters before the \":\" should be set as the tag type\n",
    "  and characters after the \":\" should be set as the tag key\n",
    "- if there are additional \":\" in the \"k\" value they and they should be ignored and kept as part of\n",
    "  the tag key. For example:\n",
    "\n",
    "  <tag k=\"addr:street:name\" v=\"Lincoln\"/>\n",
    "  should be turned into\n",
    "  {'id': 12345, 'key': 'street:name', 'value': 'Lincoln', 'type': 'addr'}\n",
    "\n",
    "- If a node has no secondary tags then the \"node_tags\" field should just contain an empty list.\n",
    "\n",
    "\n",
    "##### If the element top level tag is \"way\":\n",
    "The dictionary should have the format {\"way\": ..., \"way_tags\": ..., \"way_nodes\": ...}\n",
    "\n",
    "The \"way\" field should hold a dictionary of the following top level way attributes:\n",
    "\n",
    "- id\n",
    "-  user\n",
    "- uid\n",
    "- version\n",
    "- timestamp\n",
    "- changeset\n",
    "\n",
    "All other attributes can be ignored\n",
    "\n",
    "The \"way_tags\" field should again hold a list of dictionaries, following the exact same rules as for \"node_tags\".\n",
    "\n",
    "Additionally, the dictionary should have a field \"way_nodes\". \"way_nodes\" should hold a list of dictionaries, one for each nd child tag.  Each dictionary should have the fields:\n",
    "\n",
    "- id: the top level element (way) id\n",
    "- node_id: the ref attribute value of the nd tag\n",
    "- position: the index starting at 0 of the nd tag i.e. what order the nd tag appears within the way element      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# schema.py\n",
    "\n",
    "schema = {\n",
    "    'node': {\n",
    "        'type': 'dict',\n",
    "        'schema': {\n",
    "            'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'lat': {'required': True, 'type': 'float', 'coerce': float},\n",
    "            'lon': {'required': True, 'type': 'float', 'coerce': float},\n",
    "            'user': {'required': True, 'type': 'string'},\n",
    "            'uid': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'version': {'required': True, 'type': 'string'},\n",
    "            'changeset': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'timestamp': {'required': True, 'type': 'string'}\n",
    "        }\n",
    "    },\n",
    "    'node_tags': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'key': {'required': True, 'type': 'string'},\n",
    "                'value': {'required': True, 'type': 'string'},\n",
    "                'type': {'required': True, 'type': 'string'}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'way': {\n",
    "        'type': 'dict',\n",
    "        'schema': {\n",
    "            'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'user': {'required': True, 'type': 'string'},\n",
    "            'uid': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'version': {'required': True, 'type': 'string'},\n",
    "            'changeset': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'timestamp': {'required': True, 'type': 'string'}\n",
    "        }\n",
    "    },\n",
    "    'way_nodes': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'node_id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'position': {'required': True, 'type': 'integer', 'coerce': int}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'way_tags': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'key': {'required': True, 'type': 'string'},\n",
    "                'value': {'required': True, 'type': 'string'},\n",
    "                'type': {'required': True, 'type': 'string'}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "OSM_PATH = osm_file\n",
    "\n",
    "NODES_PATH = \"csv_files/nodes.csv\"\n",
    "NODE_TAGS_PATH = \"csv_files/nodes_tags.csv\"\n",
    "WAYS_PATH = \"csv_files/ways.csv\"\n",
    "WAY_NODES_PATH = \"csv_files/ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"csv_files/ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "def get_tags(element, element_id, problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    tags = []\n",
    "    for tag in element.iter('tag'):\n",
    "        tags_dict = {}\n",
    "        key = tag.get('k')\n",
    "        if re.search(problem_chars, key) is None:\n",
    "            tags_dict[\"id\"] = element_id\n",
    "            if \":\" in key:\n",
    "                key = key.split(\":\", 1)\n",
    "                tags_dict['key']=str(key[1])\n",
    "                tags_dict['type']=str(key[0])\n",
    "            else:\n",
    "                tags_dict['key']=str(key)\n",
    "                tags_dict['type']=default_tag_type\n",
    "            if tags_dict['type'] == 'addr' or tags_dict['type'] == 'tiger':\n",
    "                value = tag.get('v')\n",
    "                tags_dict['value'] = update_addr(tags_dict['key'], value, mapping, expected)\n",
    "            else:\n",
    "                tags_dict['value']=tag.get('v')\n",
    "            tags.append(tags_dict)\n",
    "    return tags\n",
    "    \n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "\n",
    "    if element.tag == \"node\":\n",
    "        for each in node_attr_fields:\n",
    "            attribute = element.get(each)\n",
    "            node_attribs[each] = attribute\n",
    "        node_id = node_attribs['id']\n",
    "        tags = get_tags(element, node_id)\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "\n",
    "    elif element.tag == 'way':\n",
    "        for each in way_attr_fields:\n",
    "            attribute = element.get(each)\n",
    "            way_attribs[each] = attribute\n",
    "        way_id = way_attribs['id']\n",
    "        tags = get_tags(element, way_id)\n",
    "        i = 0\n",
    "        for node in element.iter('nd'):\n",
    "            nodes_dict = {}\n",
    "            nodes_dict['id'] = way_id\n",
    "            nodes_dict['node_id'] = node.get('ref')\n",
    "            nodes_dict['position'] = i\n",
    "            way_nodes.append(nodes_dict)\n",
    "            i +=1\n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=schema):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "        codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "        codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "        codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "        codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                \n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "\n",
    "process_map(OSM_PATH, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
